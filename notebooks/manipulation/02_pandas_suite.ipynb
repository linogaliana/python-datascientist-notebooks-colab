{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Manipuler des données avec Pandas\n",
        "\n",
        "Lino Galiana  \n",
        "2025-06-14\n",
        "\n",
        "<div class=\"badge-container\"><div class=\"badge-text\">Pour essayer les exemples présents dans ce tutoriel :</div><a href=\"https://github.com/linogaliana/python-datascientist-notebooks/blob/main/notebooks/manipulation/02_pandas_suite.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/static/v1?logo=github&label=&message=View%20on%20GitHub&color=181717\" alt=\"View on GitHub\"></a>\n",
        "<a href=\"https://datalab.sspcloud.fr/launcher/ide/vscode-python?autoLaunch=true&name=«02_pandas_suite»&init.personalInit=«https%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fpython-datascientist%2Fmain%2Fsspcloud%2Finit-vscode.sh»&init.personalInitArgs=«manipulation%2002_pandas_suite%20correction»\" target=\"_blank\" rel=\"noopener\"><img src=\"https://custom-icon-badges.demolab.com/badge/SSP%20Cloud-Lancer_avec_VSCode-blue?logo=vsc&logoColor=white\" alt=\"Onyxia\"></a>\n",
        "<a href=\"https://datalab.sspcloud.fr/launcher/ide/jupyter-python?autoLaunch=true&name=«02_pandas_suite»&init.personalInit=«https%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fpython-datascientist%2Fmain%2Fsspcloud%2Finit-jupyter.sh»&init.personalInitArgs=«manipulation%2002_pandas_suite%20correction»\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/SSP%20Cloud-Lancer_avec_Jupyter-orange?logo=Jupyter&logoColor=orange\" alt=\"Onyxia\"></a>\n",
        "<a href=\"https://colab.research.google.com/github/linogaliana/python-datascientist-notebooks-colab//blob/main//notebooks/manipulation/02_pandas_suite.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a><br></div>\n",
        "\n",
        "<div class=\"alert alert-info\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\">Version 🇫🇷</h3>\n",
        "This is the French version 🇫🇷 of that chapter,to see the English version go <a href=\"https://pythonds.linogaliana.fr/content/manipulation/02_pandas_suite.html\">there</a>\n",
        "</div>\n",
        "\n",
        ":\n",
        "\n",
        ":\n",
        "\n",
        "# 1. Introduction\n",
        "\n",
        "Le [chapitre d’introduction à `Pandas`](../../content/manipulation/02_pandas_intro.qmd) a permis de présenter le principe de données organisées sous une forme de *DataFrame* et la praticité de l’écosystème `Pandas` pour effectuer des opérations simples sur un jeu de données.\n",
        "\n",
        "Il est rare de travailler exclusivement sur une source brute. Un jeu de données prend généralement de la valeur lorsqu’il est comparé à d’autres sources. Pour des chercheurs, cela permettra de contextualiser l’information présente dans une source en la comparant ou en l’associant à d’autres sources. Pour des *data scientists* dans le secteur privé, il s’agira souvent d’associer des informations sur une même personne dans plusieurs bases clientes ou comparer les clients entre eux.\n",
        "\n",
        "L’un des apports des outils modernes de *data science*, notamment `Pandas` est la simplicité par laquelle ils permettent de restructurer des sources pour travailler sur plusieurs données sur un projet.\n",
        "Ce chapitre consolide ainsi les principes vus précédemment en raffinant les traitements faits sur les données. Il va explorer principalement deux types d’opérations:\n",
        "\n",
        "-   les statistiques descriptives par groupe ;\n",
        "-   l’association de données par des caractéristiques communes.\n",
        "\n",
        "Effectuer ce travail de manière simple, fiable et efficace est indispensable pour les *data scientists* tant cette tâche est courante. Heureusement `Pandas` permet de faire cela très bien avec des données structurées. Nous verrons dans les prochains chapitres, mais aussi dans l’ensemble de la [partie sur le traitement des données textuelles](../../content/nlp/index.qmd), comment faire avec des données moins structurées.\n",
        "\n",
        "Grâce à ce travail, nous allons approfondir notre compréhension d’un phénomène réel par le biais de statistiques descriptives fines. Cela est une étape indispensable avant de basculer vers la [statistique inférentielle](https://fr.wikipedia.org/wiki/Inf%C3%A9rence_statistique#:~:text=L'inf%C3%A9rence%20statistique%20est%20l,%3A%20la%20probabilit%C3%A9%20d'erreur.), l’approche qui consiste à formaliser et généraliser des liens de corrélation ou de causalité entre des caractéristiques observées et un phénomène.\n",
        "\n",
        ":\n",
        "\n",
        ":\n",
        "\n",
        "## 1.1 Environnement\n",
        "\n",
        "Le chapitre précédent utilisait quasi exclusivement la librairie `Pandas`. Nous allons dans ce chapitre utiliser d’autres *packages* en complément de celui-ci.\n",
        "\n",
        "Comme expliqué ci-dessous, nous allons utiliser une librairie nommée `pynsee` pour récupérer les données de l’Insee utiles à enrichir notre jeu de données de l’Ademe. Cette librairie n’est pas installée par défaut dans `Python`. Avant de pouvoir l’utiliser,\n",
        "il est nécessaire de l’installer, comme la librairie `great_tables` que nous verrons à la fin de ce chapitre:"
      ],
      "id": "168de90e-6bb4-4343-8b08-825dc15bf924"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install xlrd\n",
        "!pip install pynsee\n",
        "!pip install great_tables"
      ],
      "id": "277b9fec"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "L’instruction `!pip install <pkg>` est une manière de faire comprendre à `Jupyter`, le moteur d’exécution derrière les *notebooks* que la commande qui suit (`pip install` ce `<pkg>`)\n",
        "est une commande système, à exécuter hors de `Python` (dans le terminal par exemple pour un système `Linux`).\n",
        "\n",
        "Les premiers *packages* indispensables pour démarrer ce chapitre sont les suivants:"
      ],
      "id": "10bc4f00-a72c-4575-8744-f7e0ab6a332b"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pynsee\n",
        "import pynsee.download"
      ],
      "id": "2f4c5e70"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pour obtenir des résultats reproductibles, on peut fixer la racine du générateur\n",
        "pseudo-aléatoire."
      ],
      "id": "0fcb8e41-9990-47b1-bc09-d9b0f556a024"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(123)"
      ],
      "id": "0b24539a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Données utilisées\n",
        "\n",
        "Ce tutoriel continue l’exploration du jeu de données du chapitre précédent:\n",
        "\n",
        "-   Les émissions de gaz à effet de serre estimées au niveau communal par l’ADEME. Le jeu de données est\n",
        "    disponible sur [data.gouv](https://www.data.gouv.fr/fr/datasets/inventaire-de-gaz-a-effet-de-serre-territorialise/#_)\n",
        "    et requêtable directement dans `Python` avec\n",
        "    [cet url](https://koumoul.com/s/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/convert) ;\n",
        "\n",
        "Les problématiques d’enrichissement de données (association d’une source à une autre à partir de caractéristiques communes) seront présentées à partir de deux sources produites par l’Insee:\n",
        "\n",
        "-   Le\n",
        "    [code officiel géographique](https://www.insee.fr/fr/statistiques/fichier/6800675/v_commune_2023.csv),\n",
        "    un référentiel\n",
        "    produit par l’Insee utilisé pour identifier les communes à partir d’un code unique, contrairement au code postal ;\n",
        "-   Les données [*Filosofi*](https://www.insee.fr/fr/metadonnees/source/serie/s1172), une source sur les revenus des Français à une échelle spatiale fine construite par l’Insee à partir des déclarations fiscales et d’informations sur les prestations sociales. En l’occurrence, nous allons utiliser les niveaux de revenu et les populations[1] au niveau communal afin de les mettre en regard de nos données d’émissions.\n",
        "\n",
        "Pour faciliter l’import de données Insee, il est recommandé d’utiliser le *package*\n",
        "[`pynsee`](https://pynsee.readthedocs.io/en/latest/) qui simplifie l’accès aux principaux jeux de données\n",
        "de l’Insee disponibles sur le site web [insee.fr](https://www.insee.fr/fr/accueil)\n",
        "ou via des API.\n",
        "\n",
        ":\n",
        "\n",
        ":\n",
        "\n",
        "# 2. Récupération des jeux de données\n",
        "\n",
        "## 2.1 Données d’émission de l’Ademe\n",
        "\n",
        "Comme expliqué au chapitre précédent, ces données peuvent être importées très simplement avec `Pandas`\n",
        "\n",
        "[1] Ideally, it would be more coherent, for demographic data, to use the [legal populations](https://www.insee.fr/fr/information/2008354), from the census. However, this base is not yet natively integrated into the `pynsee` library that we will use in this chapter. An open exercise is proposed to construct population aggregates from anonymized individual census data (the [detailed files](https://www.insee.fr/fr/information/2383306))."
      ],
      "id": "a441c201-32c2-4822-8c44-2853726d96ed"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://koumoul.com/s/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/convert\"\n",
        "emissions = pd.read_csv(url)\n",
        "emissions.head(2)"
      ],
      "id": "6bdcc3a0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nous allons d’ores et déjà conserver le nom des secteurs émetteurs présents dans la base de données pour simplifier des utilisations ultérieures:"
      ],
      "id": "270d95ba-65b0-471f-b853-721b40c83f83"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "secteurs = emissions.select_dtypes(include='number').columns"
      ],
      "id": "72647a54"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Les exploitations ultérieures de ces données utiliseront la dimension départementale dont nous avons montré la construction au chapitre précédent:"
      ],
      "id": "ee908130-3081-46e6-9116-0c0fb7c41901"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "emissions['dep'] = emissions[\"INSEE commune\"].str[:2]"
      ],
      "id": "cd300ae2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Données *Filosofi*\n",
        "\n",
        "On va utiliser les données Filosofi (données de revenus) au niveau communal de 2016.\n",
        "Ce n’est pas la même année que les données d’émission de CO2, ce n’est donc pas parfaitement rigoureux,\n",
        "mais cela permettra tout de même d’illustrer\n",
        "les principales fonctionnalités de `Pandas`\n",
        "\n",
        "Le point d’entrée principal de la fonction `pynsee` est la fonction `download_file`.\n",
        "\n",
        "Le code pour télécharger les données est le suivant :"
      ],
      "id": "62189a6e-782e-46a9-9bde-895c5d777f77"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pynsee.download import download_file\n",
        "filosofi = download_file(\"FILOSOFI_COM_2016\")"
      ],
      "id": "9d0a7f63"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Le *DataFrame* en question a l’aspect suivant :"
      ],
      "id": "c9c1dba7-9e05-488b-95bc-75e9a4448e83"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "filosofi.sample(3)"
      ],
      "id": "2bf6476d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`Pandas` a géré automatiquement les types de variables. Il le fait relativement bien, mais une vérification est toujours utile pour les variables qui ont un statut spécifique.\n",
        "\n",
        "Pour les variables qui ne sont pas en type `float` alors qu’elles devraient l’être, on modifie leur type."
      ],
      "id": "f70cf97c-3a5a-42aa-ab10-2b2407ba7177"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "filosofi = (\n",
        "  filosofi\n",
        "  .astype(\n",
        "    {c: \"float\" for c in filosofi.columns[2:]}\n",
        "  )\n",
        ")"
      ],
      "id": "727d8db4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Un simple coup d’oeil sur les données\n",
        "donne une idée assez précise de la manière dont les données sont organisées.\n",
        "On remarque que certaines variables de `filosofi` semblent avoir beaucoup de valeurs manquantes (secret statistique)\n",
        "alors que d’autres semblent complètes.\n",
        "Si on désire exploiter `filosofi`, il faut faire attention à la variable choisie.\n",
        "\n",
        "Notre objectif à terme va être de relier l’information contenue entre ces\n",
        "deux jeux de données. En effet, sinon, nous risquons d’être frustré : nous allons\n",
        "vouloir en savoir plus sur les émissions de gaz carbonique mais seront très\n",
        "limités dans les possibilités d’analyse sans ajout d’une information annexe\n",
        "issue de `filosofi`.\n",
        "\n",
        "# 3. Statistiques descriptives par groupe\n",
        "\n",
        "## 3.1 Principe\n",
        "\n",
        "Nous avons vu, lors du chapitre précédent, comment obtenir\n",
        "une statistique agrégée simplement grâce à `Pandas`.\n",
        "Il est néanmoins commun d’avoir des données avec des strates\n",
        "intermédiaires d’analyse pertinentes: des variables géographiques, l’appartenance à des groupes socio-démographiques liés à des caractéristiques renseignées, des indicatrices de période temporelle, etc.\n",
        "Pour mieux comprendre la structure de ses données, les *data scientists* sont donc souvent amenés à construire des statistiques descriptives sur des sous-groupes présents dans les données. Pour reprendre l’exemple sur les émissions, nous avions précédemment construit des statistiques d’émissions au niveau national. Mais qu’en est-il du profil d’émission des différents départements ? Pour répondre à cette question, il sera utile d’agréger nos données au niveau départemental. Ceci nous donnera une information différente du jeu de données initial (niveau communal) et du niveau le plus agrégé (niveau national).\n",
        "\n",
        "En `SQL`, il est très simple de découper des données pour\n",
        "effectuer des opérations sur des blocs cohérents et recollecter des résultats\n",
        "dans la dimension appropriée.\n",
        "La logique sous-jacente est celle du *split-apply-combine* qui est repris\n",
        "par les langages de manipulation de données, auxquels `pandas`\n",
        "[ne fait pas exception](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html).\n",
        "\n",
        "L’image suivante, issue de\n",
        "[ce site](https://unlhcc.github.io/r-novice-gapminder/16-plyr/),\n",
        "représente bien la manière dont fonctionne l’approche\n",
        "`split`-`apply`-`combine`:\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://unlhcc.github.io/r-novice-gapminder/fig/12-plyr-fig1.png\" alt=\"Split-apply-combine (Source: unlhcc.github.io)\" />\n",
        "<figcaption aria-hidden=\"true\">Split-apply-combine (Source: <a href=\"https://unlhcc.github.io/r-novice-gapminder/16-plyr/\">unlhcc.github.io</a>)</figcaption>\n",
        "</figure>\n",
        "\n",
        "En `Pandas`, on utilise `groupby` pour découper les données selon un ou\n",
        "plusieurs axes (ce [tutoriel](https://realpython.com/pandas-groupby/) sur le sujet\n",
        "est particulièrement utile).\n",
        "L’ensemble des opérations d’agrégation (comptage, moyennes, etc.) que nous avions vues précédemment peut être mise en oeuvre par groupe.\n",
        "\n",
        "Techniquement, cette opération consiste à créer une association\n",
        "entre des labels (valeurs des variables de groupe) et des\n",
        "observations. Utiliser la méthode `groupby` ne déclenche pas d’opérations avant la mise en oeuvre d’une statistique, cela créé seulement une relation formelle entre des observations et des regroupemens qui seront utilisés *a posteriori*:"
      ],
      "id": "51653b47-5ba3-49c4-b408-cda40c42d79e"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "filosofi[\"dep\"] = filosofi[\"CODGEO\"].str[:2]\n",
        "filosofi.groupby('dep').__class__"
      ],
      "id": "73fe9ab3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tant qu’on n’appelle pas une action sur un `DataFrame` par groupe, du type\n",
        "`head` ou `display`, `pandas` n’effectue aucune opération. On parle de\n",
        "*lazy evaluation*. Par exemple, le résultat de `df.groupby('dep')` est\n",
        "une transformation qui n’est pas encore évaluée :"
      ],
      "id": "adbabbd8-6cae-48ab-9853-40583d01a78d"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "filosofi.groupby('dep')"
      ],
      "id": "ba23bdb6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 Illustration 1: dénombrement par groupe\n",
        "\n",
        "Pour illustrer l’application de ce principe à un comptage, on peut dénombrer le nombre de communes par département en 2023 (chaque année cette statistique change du fait des fusions de communes). Pour cela, il suffit de prendre le référentiel des communes françaises issu du code officiel géographique (COG) et dénombrer par département grâce à `count`:"
      ],
      "id": "c05ca5de-3556-40d8-8019-03513d3d93d7"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from io import StringIO\n",
        "import pandas as pd\n",
        "\n",
        "url_cog_2023 = \"https://www.insee.fr/fr/statistiques/fichier/6800675/v_commune_2023.csv\"\n",
        "url_backup = \"https://minio.lab.sspcloud.fr/lgaliana/data/python-ENSAE/cog_2023.csv\"\n",
        "\n",
        "# Try-except clause to avoid timout issue sometimes\n",
        "# Without timeout problem, pd.read_csv(url_cog_2023) would be sufficient\n",
        "try:\n",
        "  response = requests.get(url_cog_2023)\n",
        "  response.raise_for_status()\n",
        "  cog_2023 = pd.read_csv(StringIO(response.text))\n",
        "except requests.exceptions.Timeout:\n",
        "  print(\"Failing back to backup\")\n",
        "  cog_2023 = pd.read_csv(url_backup)"
      ],
      "id": "5407bee7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Grâce à ce jeu de données, sans avoir recours aux statistiques par groupe, on peut déjà savoir combien on a, respectivement, de communes, départements et régions en France:"
      ],
      "id": "f6e3a2d5-9336-4e3e-b6e9-daf11e62dee7"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "communes = cog_2023.loc[cog_2023['TYPECOM']==\"COM\"]\n",
        "communes.loc[:, ['COM', 'DEP', 'REG']].nunique()"
      ],
      "id": "29800484"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Maintenant, intéressons nous aux départements ayant le plus de communes. Il s’agit de la même fonction de dénombrement où on joue, cette fois, sur le groupe à partir duquel est calculé la statistique.\n",
        "\n",
        "Calculer cette statistique se fait de manière assez transparente lorsqu’on connaît le principe d’un calcul de statistiques avec `Pandas`:"
      ],
      "id": "64dcc81a-b211-4196-910d-acd21a011515"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "communes = cog_2023.loc[cog_2023['TYPECOM']==\"COM\"]\n",
        "communes.groupby('DEP').agg({'COM': 'nunique'})"
      ],
      "id": "ecc0b284"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En SQL, on utiliserait la requête suivante:\n",
        "\n",
        "``` sql\n",
        "SELECT dep, COUNT DISTINCT \"COM\" AS COM\n",
        "FROM communes\n",
        "GROUP BY dep\n",
        "WHERE TYPECOM == 'COM';\n",
        "```\n",
        "\n",
        "La sortie est une `Serie` indexée. Ce n’est pas très pratique comme nous avons pu l’évoquer au cours du chapitre précédent. Il est plus pratique de transformer cet objet en `DataFrame` avec `reset_index`. Enfin, avec `sort_values`, on obtient la statistique désirée:"
      ],
      "id": "3357cf65-0a54-46fc-b01e-581e83239fe5"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "(\n",
        "    communes\n",
        "    .groupby('DEP')\n",
        "    .agg({'COM': 'nunique'})\n",
        "    .reset_index()\n",
        "    .sort_values('COM', ascending = False)\n",
        ")"
      ],
      "id": "226b3825"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.3 Illustration 2: agrégats par groupe\n",
        "\n",
        "Pour illustrer les agrégats par groupe nous pouvons prendre le jeu de données de l’Insee `filosofi` et compter la population grâce à la variable `NBPERSMENFISC16`.\n",
        "\n",
        "Pour calculer le total au niveau France entière nous pouvons faire de deux manières :"
      ],
      "id": "40902d3b-c355-4de6-89b2-2125f76898cb"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "filosofi['NBPERSMENFISC16'].sum()* 1e-6"
      ],
      "id": "bba37068"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "filosofi.agg({\"NBPERSMENFISC16\": \"sum\"}).div(1e6)"
      ],
      "id": "760dfdbd"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "où les résultats sont reportés en millions de personnes. La logique est identique lorsqu’on fait des statistiques par groupe, il s’agit seulement de remplacer `filosofi` par `filosofi.groupby('dep')` pour créer une version partitionnée par département de notre jeu de données:"
      ],
      "id": "00764a19-fa81-4478-b1cb-4859dd7f6de3"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "filosofi.groupby('dep')['NBPERSMENFISC16'].sum()"
      ],
      "id": "0f1df640"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "filosofi.groupby('dep').agg({\"NBPERSMENFISC16\": \"sum\"})"
      ],
      "id": "1ef098d6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La seconde approche est plus pratique car elle donne directement un `DataFrame` `Pandas` et non une série indexée sans nom. A partir de celle-ci, quelques manipulations basiques peuvent suffire pour avoir un tableau diffusables sur la démographie départementale. Néanmoins, celui-ci, serait quelques peu brut de décoffrage car nous ne possédons à l’heure actuelle que les numéros de département. Pour avoir le nom de départements, il faudrait utiliser une deuxième base de données et croiser les informations communes entre elles (en l’occurrence le numéro du département). C’est l’objet de la prochaine partie.\n",
        "\n",
        "## 3.4 Exercice d’application\n",
        "\n",
        "Cet exercice d’application s’appuie sur le jeu de données de l’Ademe nommé `emissions` précédemment.\n",
        "\n",
        ":\n",
        "\n",
        ":\n",
        "\n",
        "A la question 1, le résultat obtenu devrait être le suivant:\n",
        "\n",
        "Ce classement reflète peut-être plus la démographie que le processus qu’on désire mesurer. Sans l’ajout d’une information annexe sur la population de chaque département pour contrôler ce facteur, on peut difficilement savoir s’il y a une différence structurelle de comportement entre les habitants du Nord (département 59) et ceux de la Moselle (département 57).\n",
        "\n",
        "A l’issue de la question 2, prenons la part des émissions de l’agriculture et du secteur tertiaire dans les émissions départementales:\n",
        "\n",
        "Ces résultats sont assez logiques ; les départements ruraux ont une part plus importante de leur émission issue de l’agriculture, les départements urbains ont plus d’émissions issues du secteur tertiaire, ce qui est lié à la densité plus importante de ces espaces.\n",
        "\n",
        "Grâce à ces statistiques on progresse dans la connaissance de notre jeu de données et donc de la nature des émissions de C02 en France.\n",
        "Les statistiques descriptives par groupe nous permettent de mieux saisir l’hétérogénéité spatiale de notre phénomène.\n",
        "\n",
        "Cependant, on reste limité dans notre capacité à interpréter les statistiques obtenues sans recourir à l’utilisation d’information annexe. Pour donner du sens et de la valeur à une statistique, il faut généralement associer celle-ci à de la connaissance annexe sous peine qu’elle soit désincarnée.\n",
        "\n",
        "Dans la suite de ce chapitre, nous envisagerons une première voie qui est le croisement avec des données complémentaires. On appelle ceci un enrichissement de données. Ces données peuvent être des observations à un niveau identique à celui de la source d’origine. Par exemple, l’un des croisements les plus communs est d’associer une base client à une base d’achats afin de mettre en regard un comportement d’achat avec des caractéristiques pouvant expliquer celui-ci. Les associations de données peuvent aussi se faire à des niveaux conceptuels différents, en général à un niveau plus agrégé pour contextualiser la donnée plus fine et comparer une observation à des mesures dans un groupe similaire. Par exemple, on peut associer des temps et des modes de transports individuels à ceux d’une même classe d’âge ou de personnes résidant dans la même commune pour pouvoir comparer la différence entre certains individus et un groupe sociodémographique similaire.\n",
        "\n",
        "# 4. Restructurer les données\n",
        "\n",
        "## 4.1 Principe\n",
        "\n",
        "Quand on a plusieurs informations pour un même individu ou groupe, on\n",
        "retrouve généralement deux types de structure de données :\n",
        "\n",
        "-   format **wide** : les données comportent des observations répétées, pour un même individu (ou groupe), dans des colonnes différentes\n",
        "-   format **long** : les données comportent des observations répétées, pour un même individu, dans des lignes différentes avec une colonne permettant de distinguer les niveaux d’observations\n",
        "\n",
        "Un exemple de la distinction entre les deux peut être pris à l’ouvrage de référence d’Hadley Wickham, [*R for Data Science*](https://r4ds.hadley.nz/):\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://d33wubrfki0l68.cloudfront.net/3aea19108d39606bbe49981acda07696c0c7fcd8/2de65/images/tidy-9.png\" alt=\"Données long et wide (Source: R for Data Science)\" />\n",
        "<figcaption aria-hidden=\"true\">Données <em>long</em> et <em>wide</em> (Source: <a href=\"https://r4ds.hadley.nz/\"><em>R for Data Science</em></a>)</figcaption>\n",
        "</figure>\n",
        "\n",
        "L’aide mémoire suivante aidera à se rappeler les fonctions à appliquer si besoin :\n",
        "\n",
        "![](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/reshape.png)\n",
        "\n",
        "Le fait de passer d’un format *wide* au format *long* (ou vice-versa)\n",
        "peut être extrêmement pratique car certaines fonctions sont plus adéquates sur une forme de données ou sur l’autre.\n",
        "\n",
        "En règle générale, avec `Python` comme avec `R`, les **formats *long* sont souvent préférables**.\n",
        "Les formats *wide* sont plutôt pensés pour des tableurs comme `Excel` ou on dispose d’un nombre réduit\n",
        "de lignes à partir duquel faire des tableaux croisés dynamiques.\n",
        "\n",
        "## 4.2 Exercice d’application\n",
        "\n",
        "Les données de l’ADEME, et celles de l’Insee également, sont au format\n",
        "*wide*.\n",
        "Le prochain exercice illustre l’intérêt de faire la conversion *long* $\\to$ *wide*\n",
        "avant de faire un graphique avec la méthode `plot` vue au chapitre précédent\n",
        "\n",
        ":\n",
        "\n",
        ":\n",
        "\n",
        "# 5. Joindre des données\n",
        "\n",
        "## 5.1 Principe\n",
        "\n",
        "Nous allons ici nous focaliser sur le cas le plus favorable qui est la situation\n",
        "où une information permet d’apparier de manière exacte deux bases de données[1].\n",
        "C’est un besoin quotidien des *data scientists* d’associer des informations présentes dans plusieurs fichiers. Par exemple, dans des bases de données d’entreprises, les informations clients (adresse, âge, etc.) seront dans un fichier, les ventes dans un autre et les caractéristiques des produits dans un troisième fichier. Afin d’avoir une base complète mettant en regard toutes ces informations, il sera dès lors nécessaire de joindre ces trois fichiers sur la base d’informations communes.\n",
        "\n",
        "Cette pratique découle du fait que de nombreux systèmes d’information prennent la forme d’un schéma en étoile:\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://www.databricks.com/wp-content/uploads/2022/04/star-schema-erd.png\" alt=\"Illustration du schéma en étoile (Source: Databricks)\" />\n",
        "<figcaption aria-hidden=\"true\">Illustration du schéma en étoile (Source: <a href=\"https://www.databricks.com/wp-content/uploads/2022/04/star-schema-erd.png\">Databricks</a>)</figcaption>\n",
        "</figure>\n",
        "\n",
        "Cette structuration de l’information est très liée au modèle des tables relationnelles des années 1980. Aujourd’hui, il existe des modèles de données plus flexibles où l’information est empilée dans un *data lake* sans structure *a priori*. Néanmoins ce modèle du schéma en étoile conserve une pertinence parce qu’il permet de partager l’information qu’à ceux qui en ont besoin laissant le soin à ceux qui ont besoin de lier des données entre elles de le faire.\n",
        "\n",
        "Puisque la logique du schéma en étoile vient historiquement des bases relationnelles, il est naturel qu’il s’agisse d’une approche intrinsèquement liée à la philosophie du SQL, jusque dans le vocabulaire. On parle souvent de jointure de données, un héritage du terme `JOIN` de SQL, et la manière de décrire les jointures (*left join*, *right join*…) est directement issue des instructions SQL associées.\n",
        "\n",
        "On parle généralement de base de gauche et de droite pour illustrer les jointures:\n",
        "\n",
        "![](https://minio.lab.sspcloud.fr/lgaliana/python-ENSAE/inputs/merge_pandas/join_initial.png)\n",
        "\n",
        "## 5.2 Mise en oeuvre avec `Pandas`\n",
        "\n",
        "En `Pandas`, la méthode la plus pratique pour associer des jeux de données à partir de caractéristiques communes est `merge`. Ses principaux arguments permettent de contrôler le comportement de jointure. Nous allons les explorer de manière visuelle.\n",
        "\n",
        "En l’occurrence, pour notre problématique de construction de statistiques\n",
        "sur les émissions de gaz carbonique, la base de gauche sera le *DataFrame* `emission` et la base de droite le *DataFrame* `filosofi`:\n",
        "\n",
        "[1] Otherwise, we enter the realm of fuzzy matching or probabilistic matching. Fuzzy matching occurs when we no longer have an exact identifier to link two databases but have partially noisy information between two sources to make the connection. For example, in a product database, we might have `Coca Cola 33CL` and in another `Coca Cola canette`, but these names hide the same product. The chapter on [Introduction to Textual Search with ElasticSearch](../../content/modern-ds/elastic.qmd) addresses this issue. Probabilistic matching is another approach. In these, observations in two databases are associated not based on an identifier but on the distance between a set of characteristics in both databases. This technique is widely used in medical statistics or in the evaluation of public policies based on [*propensity score matching*](https://en.wikipedia.org/wiki/Propensity_score_matching)."
      ],
      "id": "0d91f18b-ecc8-4b77-a0db-b48f0f59993d"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "emissions.head(2)"
      ],
      "id": "804b0e21"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "filosofi.head(2)"
      ],
      "id": "b5b7d471"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On parle de clé(s) de jointure pour nommer la ou les variable(s) nécessaire(s) à la fusion de données. Ce sont les variables communes aux deux jeux de données. Il n’est pas nécessaire qu’elles aient le même nom en revanche elles doivent partager des valeurs communes autrement l’intersection entre ces deux bases est l’ensemble vide.\n",
        "\n",
        "On peut jouer sur deux dimensions dans la jointure (ceci sera plus clair ensuite avec les exemples graphiques).\n",
        "\n",
        "-   Il existe principalement trois types de fusions: *left join* et *right join* ou un combo des deux selon le type de pivot qu’on désire mettre en oeuvre.\n",
        "-   Ensuite, il existe deux manières de fusionner les valeurs une fois qu’on a choisi un pivot: *inner* ou *outer join*. Dans le premier cas, on ne conserve que les observations où les clés de jointures sont présentes dans les deux bases, dans le second on conserve toutes les observations de la clé de jointure des variables pivot quitte à avoir des valeurs manquantes si la deuxième base de données n’a pas de telles observations.\n",
        "\n",
        "Dans les exemples ci-dessous, nous allons utiliser les codes communes et les départements comme variables de jointure. En soi, l’usage du département n’est pas nécessaire puisqu’il se déduit directement du code commune mais cela permet d’illustrer le principe des jointures sur plusieurs variables. A noter que le nom de la commune est volontairement mis de côté pour effectuer des jointures alors que c’est une information commune aux deux bases. Cependant, comme il s’agit d’un champ textuel, dont le formattage peut suivre une norme différente dans les deux bases, ce n’est pas une information fiable pour faire une jointure exacte.\n",
        "\n",
        "Pour illustrer le principe du pivot à gauche ou à droite, on va créer deux variables identificatrices de la ligne de nos jeux de données de gauche et de droite. Cela nous permettra de trouver facilement les lignes présentes dans un jeu de données mais pas dans l’autre."
      ],
      "id": "02d2c7cf-e2fe-465f-a254-19b5fe8392d9"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "emissions = emissions.reset_index(names = ['id_left'])\n",
        "filosofi = filosofi.reset_index(names = ['id_right'])"
      ],
      "id": "c2c57beb"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2.1 *Left join*\n",
        "\n",
        "Commençons avec la jointure à gauche. Comme son nom l’indique, on va prendre la variable de gauche en pivot:\n",
        "\n",
        "![](https://minio.lab.sspcloud.fr/lgaliana/python-ENSAE/inputs/merge_pandas/left_join.png)"
      ],
      "id": "6c9a1f6d-918e-4df6-875d-ccf39c920d3a"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "left_merged = emissions.merge(\n",
        "  filosofi,\n",
        "  left_on = [\"INSEE commune\", \"dep\"],\n",
        "  right_on = [\"CODGEO\", \"dep\"],\n",
        "  how = \"left\"\n",
        ")\n",
        "left_merged.head(3)"
      ],
      "id": "d41d66c8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Il est recommandé de toujours expliciter les clés de jointures par le biais des arguments `left_on`, `right_on` ou `on` si les noms de variables sont communs dans les deux bases.\n",
        "Si on a des noms de variables communes entre les bases mais qu’elles ne sont pas définies comme clés de jointures, celles-ci ne seront pas utilisées pour joindre mais seront conservées avec un suffixe qui par défaut est `_x` et `_y` (paramétrable par le biais de l’argument `suffixes`).\n",
        "\n",
        "La syntaxe `Pandas` étant directement inspirée de SQL, on a une traduction assez transparente de l’instruction ci-dessus en SQL:\n",
        "\n",
        "``` sql\n",
        "SELECT *\n",
        "FROM emissions\n",
        "LEFT JOIN filosofi\n",
        "  ON emissions.`INSEE commune` = filosofi.CODGEO\n",
        "  AND emissions.dep = filosofi.dep;\n",
        "```\n",
        "\n",
        "En faisant une jointure à gauche, on doit en principe avoir autant de lignes que la base de données à gauche:"
      ],
      "id": "248a2918-630a-4cbd-95ad-93f20fd3f224"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "left_merged.shape[0] == emissions.shape[0]"
      ],
      "id": "c10cc9a3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Autrement, cela est signe qu’il y a une clé dupliquée à droite. Grâce à notre variable `id_right`, on peut savoir les codes communes à droite qui n’existent pas à gauche:"
      ],
      "id": "a61ca45e-c793-4f5b-8c38-6072f319df80"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "left_merged.loc[left_merged['id_right'].isna()].tail(3)"
      ],
      "id": "5ab435f6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cela vient du fait que nous utilisons des données qui ne sont pas de la même année de référence du code officiel géographique (2016 vs 2018). Pendant cet intervalle, il y a eu des changements de géographie, notamment des fusions de communes. Par exemple, la commune de Courcouronnes qu’on a vu ci-dessus peut être retrouvée regroupée avec Evry dans le jeu de données filosofi (base de droite):"
      ],
      "id": "5dfd4f1e-bc7e-46ee-912a-a9cec2e97e55"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "filosofi.loc[\n",
        "  filosofi['LIBGEO']\n",
        "  .str.lower()\n",
        "  .str.contains(\"courcouronnes\")\n",
        "]"
      ],
      "id": "c1541739"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dans un exercice de construction de statistiques publiques, on ne pourrait donc se permettre cette disjonction des années.\n",
        "\n",
        "### 5.2.2 *Right join*\n",
        "\n",
        "![](https://minio.lab.sspcloud.fr/lgaliana/python-ENSAE/inputs/merge_pandas/right_join.png)\n",
        "\n",
        "Le principe est le même mais cette fois c’est la base de droite qui est prise sous forme de pivot:"
      ],
      "id": "6a5685a3-edcb-402c-9d09-1e4003b188dd"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "right_merged = emissions.merge(\n",
        "  filosofi,\n",
        "  left_on = [\"INSEE commune\", \"dep\"],\n",
        "  right_on = [\"CODGEO\", \"dep\"],\n",
        "  how = \"right\"\n",
        ")\n",
        "right_merged.head(3)"
      ],
      "id": "bf2f35b6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "L’instruction équivalente en SQL serait\n",
        "\n",
        "``` sql\n",
        "SELECT *\n",
        "FROM filosofi\n",
        "RIGHT JOIN emissions\n",
        "  ON filosofi.CODGEO = emissions.`INSEE commune`\n",
        "  AND filosofi.dep = emissions.dep;\n",
        "```\n",
        "\n",
        "On peut, comme précédemment, vérifier la cohérence des dimensions:"
      ],
      "id": "40825d38-a93a-48f7-80cc-de3ed3ed54d4"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "right_merged.shape[0] == filosofi.shape[0]"
      ],
      "id": "c45f0831"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pour vérifier le nombre de lignes des données Filosofi que nous n’avons pas dans notre jeu d’émissions de gaz carbonique, on peut faire"
      ],
      "id": "9f8f5a8f-1727-4660-ab3f-2fa199bc139d"
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "right_merged['id_left'].isna().sum()"
      ],
      "id": "8e42aa83"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "C’est un nombre faible. Quelles sont ces observations ?"
      ],
      "id": "c6a78a8e-3146-4d0d-8582-a96c65614f19"
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "right_merged.loc[\n",
        "  right_merged['id_left'].isna(),\n",
        "  filosofi.columns.tolist() + emissions.columns.tolist()\n",
        "]"
      ],
      "id": "2ff50dad"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Il est suprenant de voir que Paris, Lyon et Marseille sont présents\n",
        "dans la base des statistiques communales mais pas dans celles des émissions.\n",
        "Pour comprendre pourquoi, recherchons dans nos données d’émissions les observations liées à Marseille:"
      ],
      "id": "4e652748-b7db-4c5e-957e-ea27d55477ca"
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "emissions.loc[\n",
        "  emissions[\"Commune\"]\n",
        "  .str.lower()\n",
        "  .str.contains('MARSEILLE')\n",
        "]"
      ],
      "id": "36aabc31"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cela vient du fait que le jeu de données des émissions de l’Ademe propose de l’information sur les arrondissements dans les trois plus grandes villes\n",
        "là où le jeu de données de l’Insee ne fait pas cette décomposition.\n",
        "\n",
        "### 5.2.3 *Inner join*\n",
        "\n",
        "![](https://minio.lab.sspcloud.fr/lgaliana/python-ENSAE/inputs/merge_pandas/inner.png)\n",
        "\n",
        "Il s’agit du jeu de données où les clés sont retrouvées à l’intersection des deux tables."
      ],
      "id": "c1e5c172-2be4-44b1-b43b-5e0772a13575"
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "inner_merged = emissions.merge(\n",
        "  filosofi,\n",
        "  left_on = [\"INSEE commune\", \"dep\"],\n",
        "  right_on = [\"CODGEO\", \"dep\"],\n",
        "  how = \"inner\"\n",
        ")\n",
        "inner_merged.head(3)"
      ],
      "id": "0b2f779d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En SQL, cela donne\n",
        "\n",
        "``` sql\n",
        "SELECT *\n",
        "FROM emissions\n",
        "INNER JOIN filosofi\n",
        "  ON emissions.`INSEE commune` = filosofi.CODGEO\n",
        "  AND emissions.dep = filosofi.dep;\n",
        "```\n",
        "\n",
        "Le nombre de lignes dans notre jeu de données peut être comparé au jeu de droite et de gauche:"
      ],
      "id": "83fcbc90-2651-49fe-a516-bc51822b360f"
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "inner_merged.shape[0] == (\n",
        "  left_merged.shape[0] - left_merged['id_right'].isna().sum()\n",
        ")"
      ],
      "id": "38cc75af"
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "inner_merged.shape[0] == (\n",
        "  right_merged.shape[0] - right_merged['id_left'].isna().sum()\n",
        ")"
      ],
      "id": "764dccca"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2.4 *Full join*\n",
        "\n",
        "Le *full join* est un pivot à gauche puis à droite pour les informations qui n’ont pas été trouvées\n",
        "\n",
        "![](https://minio.lab.sspcloud.fr/lgaliana/python-ENSAE/inputs/merge_pandas/full_join.png)"
      ],
      "id": "5b09a5fb-4b38-4af0-a7af-32f00f6c7224"
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "full_merged = emissions.merge(\n",
        "  filosofi,\n",
        "  left_on = [\"INSEE commune\", \"dep\"],\n",
        "  right_on = [\"CODGEO\", \"dep\"],\n",
        "  how = \"outer\"\n",
        ")\n",
        "full_merged.head(3)"
      ],
      "id": "cf23f8e2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comme d’habitude, la traduction en SQL est presque immédiate:\n",
        "\n",
        "``` sql\n",
        "SELECT *\n",
        "FROM emissions\n",
        "FULL OUTER JOIN filosofi\n",
        "  ON emissions.`INSEE commune` = filosofi.CODGEO\n",
        "  AND emissions.dep = filosofi.dep;\n",
        "```\n",
        "\n",
        "Cette fois, on a une combinaison de nos trois jeux de données initiaux:\n",
        "\n",
        "-   Le *inner join* ;\n",
        "-   Le *left join* sur les observations sans clé de droite ;\n",
        "-   Le *right join* sur les observations sans clé de gauche ;"
      ],
      "id": "f60a7f8a-6444-424e-bfc3-7a4646cee4f2"
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "(\n",
        "  full_merged['id_left'].isna().sum() + full_merged['id_right'].isna().sum()\n",
        ") == (\n",
        "  left_merged['id_right'].isna().sum() + right_merged['id_left'].isna().sum()\n",
        ")"
      ],
      "id": "c1569899"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2.5 En résumé\n",
        "\n",
        "![](https://external-preview.redd.it/yOLzCR0qSzul2WpjQorxINB0xpU3_N9twmFVsgbGJwQ.jpg?auto=webp&s=4feedc91302ba635b3028a21b98d047def5cdc2b)\n",
        "\n",
        "## 5.3 Exemples d’identifiants dans les données françaises\n",
        "\n",
        "### 5.3.1 Le Code officiel géographique (COG): l’identifiant des données géographiques\n",
        "\n",
        "Pour les données géographiques, il existe de nombreux identifiants selon la problématique d’étude.\n",
        "Parmi les besoins principaux, on retrouve le fait d’apparier des données géographiques à partir d’un identifiant administratif commun. Par exemple, associer deux jeux de données au niveau communal.\n",
        "\n",
        "Pour cela, l’identifiant de référence est le code Insee, issu du [Code officiel géographique (COG)](https://www.insee.fr/fr/information/2560452) que nous utilisons depuis le dernier chapitre et que nous aurons amplement l’occasion d’exploiter au cours des différents chapitres de ce cours.\n",
        "La géographie administrative étant en évolution perpétuelle, la base des code Insee est une base vivante. Le site et les API de l’Insee permettent de récupérer l’historique d’après-guerre afin de pouvoir faire de l’analyse géographique sur longue période.\n",
        "\n",
        "Les codes postaux ne peuvent être considérés comme un identifiant : ils peuvent regrouper plusieurs communes ou, au contraire, une même commune peut avoir plusieurs codes postaux. Il s’agit d’un système de gestion de la Poste qui n’a pas été construit pour l’analyse statistique.\n",
        "\n",
        "Pour se convaincre du problème, à partir des données mises à disposition par La Poste, on peut voir que le code postal 11420 correspond à 11 communes:"
      ],
      "id": "b0fd0ba0-8172-47df-a385-6fd4568d9b06"
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "codes_postaux = pd.read_csv(\n",
        "  \"https://datanova.laposte.fr/data-fair/api/v1/datasets/laposte-hexasmal/raw\",\n",
        "  sep = \";\", encoding = \"latin1\",\n",
        "  dtype = {\"Code_postal\": \"str\", \"#Code_commune_INSEE\": \"str\"}\n",
        ")\n",
        "codes_postaux.loc[codes_postaux['Code_postal'] == \"11420\"]"
      ],
      "id": "7b308220"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En anticipant sur les compétences développées lors des prochains chapitres, nous pouvons représenter le problème sous forme cartographique en prenant l’exemple de l’Aude. Le code pour produire la carte des codes communes est donné tel quel, il n’est pas développé car il fait appel à des concepts et librairies qui seront présentés lors du prochain chapitre:"
      ],
      "id": "f15ba674-0bdb-4893-825c-c91cba6857fc"
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "from cartiflette import carti_download\n",
        "shp_communes = carti_download(\n",
        "  values = [\"11\"],\n",
        "  crs = 4326,\n",
        "  borders = \"COMMUNE\",\n",
        "  simplification=50,\n",
        "  filter_by=\"DEPARTEMENT\",\n",
        "  source=\"EXPRESS-COG-CARTO-TERRITOIRE\",\n",
        "  year=2022)\n",
        "\n",
        "codes_postaux11 = shp_communes.merge(\n",
        "  codes_postaux,\n",
        "  left_on = \"INSEE_COM\",\n",
        "  right_on = \"#Code_commune_INSEE\"\n",
        ")\n",
        "codes_postaux11 = codes_postaux11.dissolve(by = \"Code_postal\")\n",
        "\n",
        "# Carte\n",
        "ax = shp_communes.plot(color='white', edgecolor='blue', linewidth = 0.5)\n",
        "ax = codes_postaux11.plot(ax = ax, color='none', edgecolor='black')\n",
        "ax.set_axis_off()"
      ],
      "id": "edaf4ffd"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3.2 Sirene: l’identifiant dans les données d’entreprises\n",
        "\n",
        "Pour relier les microdonnées d’entreprises françaises, il existe un numéro unique d’identification : le [numéro `Siren`](https://entreprendre.service-public.fr/vosdroits/F32135). Il s’agit d’un numéro d’identification dans un répertoire légal d’entreprise indispensable pour toutes démarches juridiques, fiscales… Pour les entreprises qui possèdent plusieurs établissements - par exemple dans plusieurs villes - il existe un identifiant dérivé qui s’appelle le [`Siret`](https://www.economie.gouv.fr/cedef/numero-siret): aux 9 chiffres du numéro Sirene s’ajoutent 5 chiffres d’identifications de l’établissement. D’ailleurs, les administrations publiques sont également concernées par le numéro Siren: étant amenées à effectuer des opérations de marchés (achat de matériel, locations de biens, etc.) elles disposent également d’un identifiant Siren. Etant inscrits dans des répertoires légaux pour lesquels les citoyens sont publics, les numéros Siren et les noms des entreprises associées sont disponibles en *open data*, par exemple sur [annuaire-entreprises.data.gouv.fr/](https://annuaire-entreprises.data.gouv.fr/) pour une recherche ponctuelle, sur [data.gouv.fr](https://www.data.gouv.fr/fr/datasets/base-sirene-des-entreprises-et-de-leurs-etablissements-siren-siret/).\n",
        "\n",
        "Cette base Sirene est une mine d’information, parfois comique, sur les entreprises françaises. Par exemple, le site [tif.hair/](https://tif.hair/) s’est amusé à répertorier la part des salons de coiffures proposant des jeux de mots dans le nom du salon. Lorsqu’un entrepreneur déclare la création d’une entreprise, il reçoit un numéro Sirene et un code d’activité (le [code APE](https://entreprendre.service-public.fr/vosdroits/F33050)) relié à la description qu’il a déclaré de l’activité de son entreprise. Ce code permet de classer l’activité d’une entreprise dans la [Nomenclature d’activités françaises (NAF)](https://www.insee.fr/fr/information/2406147) ce qui servira à l’Insee pour la publication de statistiques sectorielles. En l’occurrence, pour les coiffeurs, le code dans la NAF est [`96.02A`](https://www.insee.fr/fr/metadonnees/nafr2/sousClasse/96.02A?champRecherche=false). Il est possible à partir de la base disponible en *open data* d’avoir en quelques lignes de `Python` la liste de tous les coiffeurs puis de s’amuser à explorer ces données (objet du prochain exercice optionnel).\n",
        "\n",
        "L’exercice suivant, optionnel, propose de s’amuser à reproduire de manière simplifiée le recensement fait par [tif.hair/](https://tif.hair/)\n",
        "des jeux de mots dans les salons de coiffure. Il permet de pratiquer quelques méthodes de manipulation textuelle, en avance de phase sur le chapitre consacré aux [expressions régulières](../../content/manipulation/04b_regex_TP.qmd).\n",
        "\n",
        "Le jeu de données de l’ensemble des entreprises étant assez volumineux (autour de 4Go en CSV après décompression), il est plus pratique de partir sur un jeu de données au format `Parquet`, plus optimisé (plus de détails sur ce format dans le [chapitre d’approfondissement](../../content/modern-ds/s3.qmd) qui lui est consacré).\n",
        "\n",
        "Pour lire ce type de fichiers de manière optimale, il est conseillé d’utiliser la librairie `DuckDB` qui permet de ne consommer que les données nécessaires et non de télécharger l’ensemble du fichier pour n’en lire qu’une partie comme ce serait le cas avec `Pandas` (voir la fin de ce chapitre, section “Aller au-delà de `Pandas`”). La requête SQL suivante se traduit en langage naturel par l’instruction suivante: *“A partir du fichier `Parquet`, je ne veux que quelques colonnes du fichier pour les coiffeurs (APE: 96.02A) dont le nom de l’entreprise (`denominationUsuelleEtablissement`) est renseigné”*:"
      ],
      "id": "7dd6f4ea-e37a-4e68-9b78-dbf93816a734"
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "import duckdb\n",
        "coiffeurs = duckdb.sql(\"\"\"\n",
        "  SELECT\n",
        "    siren, siret, dateDebut, enseigne1Etablissement, activitePrincipaleEtablissement, denominationUsuelleEtablissement\n",
        "  FROM\n",
        "    read_parquet('https://minio.lab.sspcloud.fr/lgaliana/data/sirene2024.parquet')\n",
        "  WHERE\n",
        "    activitePrincipaleEtablissement == '96.02A'\n",
        "    AND\n",
        "    denominationUsuelleEtablissement IS NOT NULL\n",
        "\"\"\")\n",
        "coiffeurs = coiffeurs.df()"
      ],
      "id": "23d65490"
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "coiffeurs.head(3)"
      ],
      "id": "6cc049d4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":\n",
        "\n",
        ":\n",
        "\n",
        "Avec la question 2, on retrouve une liste de jeux de mots assez imaginatifs à partir du terme `tif`:"
      ],
      "id": "f5be79ab-44a8-48a1-bf20-e7f8b40f2a18"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {}
        }
      ],
      "source": [],
      "id": "aea2ef4d-f430-463a-a543-0ea7bbfe15eb"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Voici sous une forme plus interactive l’ensemble des coiffeurs qui possèdent les termes `tif` dans le nom de leur entreprise déposée dans les données officielles:\n",
        "\n",
        "Bien sûr, pour aller plus loin, il faudrait mieux normaliser les données, vérifier que l’information recherchée n’est pas à cheval sur plusieurs colonnes et bien sûr faire de l’inspection visuelle pour détecter les jeux de mots cachés. Mais déjà, en quelques minutes, on a des statistiques partielles sur le phénomène des coiffeurs blagueurs.\n",
        "\n",
        "### 5.3.3 Le NIR et la question de la confidentialité des identifiants individuels\n",
        "\n",
        "En ce qui concerne les individus, il existe un identifiant unique permettant de relier ceux-ci dans différentes sources de données : le [NIR](https://www.cnil.fr/fr/definition/nir-numero-dinscription-au-repertoire), aussi connu sous le nom de numéro Insee ou numéro de sécurité sociale.\n",
        "Ce numéro est nécessaire à l’administration pour la gestion des droits à prestations sociales (maladie, vieillesse, famille…). Au-delà de cette fonction qui peut être utile au quotidien, ce numéro est un identifiant individuel unique dans le [Répertoire national d’identification des personnes physiques (RNIPP)](https://www.insee.fr/fr/metadonnees/definition/c1602).\n",
        "\n",
        "Cet identifiant est principalement présent dans des bases de gestion, liées aux fiches de paie, aux prestations sociales, etc. Cependant, *a contrario* du numéro Sirene, celui-ci contient en lui-même plusieurs informations sensibles - en plus d’être intrinsèquement relié à la problématique sensible des droits à la sécurité sociale.\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://www.ameli.fr/sites/default/files/styles/webp_ckeditor/public/thumbnails/image/infographie_assures-regle-identification-assures.gif.webp?itok=j2owVDrB\" alt=\"Le numéro de sécurité sociale (Source: Améli)\" />\n",
        "<figcaption aria-hidden=\"true\">Le numéro de sécurité sociale (Source: <a href=\"https://www.ameli.fr/assure/droits-demarches/principes/numero-securite-sociale\">Améli</a>)</figcaption>\n",
        "</figure>\n",
        "\n",
        "Pour pallier ce problème, a récémment été mis en oeuvre le [code statistique non signifiant (CSNS)](https://www.insee.fr/fr/information/7635825?sommaire=7635842) ou NIR haché, un identifiant individuel anonyme non identifiant. L’objectif de cet identifiant anonymisé est de réduire la dissémination d’une information personnelle qui permettait certes aux fonctionnaires et chercheurs de relier de manière déterministe de nombreuses bases de données mais donnait une information non indispensable aux analystes sur les personnes en question.\n",
        "\n",
        "## 5.4 Exercices d’application\n",
        "\n",
        "### 5.4.1 Pourquoi a-t-on besoin d’un code commune quand on a déjà son nom ?\n",
        "\n",
        "Cet exercice va revenir un peu en arrière afin de saisir pourquoi nous avons pris comme hypothèse ci-dessus que le code commune était la clé de jointure.\n",
        "\n",
        ":\n",
        "\n",
        ":\n",
        "\n",
        "Ce petit exercice permet donc de se rassurer car les libellés dupliqués\n",
        "sont en fait des noms de commune identiques mais qui ne sont pas dans le même département.\n",
        "Il ne s’agit donc pas d’observations dupliquées.\n",
        "On peut donc se fier aux codes communes, qui eux sont uniques.\n",
        "\n",
        "### 5.4.2 Calculer une empreinte carbone grâce à l’association entre des sources\n",
        "\n",
        ":\n",
        "\n",
        ":\n",
        "\n",
        "A l’issue de la question 5, le graphique des corrélations est le suivant :\n",
        "\n",
        "# 6. Formatter des tableaux de statistiques descriptives\n",
        "\n",
        "Un *dataframe* `Pandas`\n",
        "est automatiquement mis en forme lorsqu’il est visualisé depuis un *notebook* sous forme de table HTML à la mise en forme minimaliste.\n",
        "Cette mise en forme est pratique pour voir\n",
        "les données, une tâche indispensable pour les *data scientists*\n",
        "mais ne permet pas d’aller vraiment au-delà.\n",
        "\n",
        "Dans une phase\n",
        "exploratoire, il peut être pratique d’avoir un tableau\n",
        "un peu plus complet, intégrant notamment des visualisations\n",
        "minimalistes, pour mieux connaître ses données. Dans la phase\n",
        "finale d’un projet, lorsqu’on communique sur un projet, il\n",
        "est avantageux de disposer d’une visualisation attrative.\n",
        "Pour ces deux besoins, les sorties des *notebooks* sont\n",
        "une réponse peu satisfaisante, en plus de nécessiter\n",
        "le *medium* du *notebook* qui peut en rebuter certains.\n",
        "\n",
        "Heureusement, le tout jeune *package* [`great_tables`](https://posit-dev.github.io/great-tables/get-started/) permet, simplement, de manière programmatique, la création de tableaux\n",
        "qui n’ont rien à envier à des productions manuelles fastidieuses faites dans `Excel`\n",
        "et difficilement répliquables. Ce *package* est un portage en `Python` du *package* [`GT`](https://gt.rstudio.com/).\n",
        "`great_tables` construit des tableaux\n",
        "*html* ce qui offre une grande richesse dans la mise en forme et permet une excellente intégration avec [`Quarto`](https://quarto.org/), l’outil de publication reproductible développé par\n",
        "L’exercice suivant proposera de construire un tableau avec\n",
        "ce *package*, pas à pas.\n",
        "\n",
        "Afin de se concentrer sur la construction du tableau,\n",
        "les préparations de données à faire en amont sont données\n",
        "directement. Nous allons repartir de ce jeu de données:\n",
        "\n",
        "Pour être sûr d’être en mesure d’effectuer le prochain exercice, voici le dataframe nécessaire pour celui-ci"
      ],
      "id": "2c822b47-6211-46f6-98fc-b8c43de9bb93"
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "emissions['emissions'] = emissions.sum(axis = 1, numeric_only = True)\n",
        "\n",
        "emissions_merged = (\n",
        "    emissions.reset_index()\n",
        "    .merge(filosofi, left_on = \"INSEE commune\", right_on = \"CODGEO\")\n",
        ")\n",
        "emissions_merged['empreinte'] = emissions_merged['emissions']/emissions_merged['NBPERSMENFISC16']\n",
        "emissions_merged['empreinte'] = emissions_merged['empreinte'].astype(float)"
      ],
      "id": "46231d02"
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "emissions_table = (\n",
        "    emissions_merged\n",
        "    .rename(columns={\"dep_y\": \"dep\", \"NBPERSMENFISC16\": \"population\", \"MED16\": \"revenu\"})\n",
        "    .groupby(\"dep\")\n",
        "    .agg({\"empreinte\": \"sum\", \"revenu\": \"median\", \"population\": \"sum\"}) #pas vraiment le revenu médian\n",
        "    .reset_index()\n",
        "    .sort_values(by = \"empreinte\")\n",
        ")"
      ],
      "id": "f7bf5608"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dans ce tableau nous allons intégrer des barres horizontales, à la manière des exemples présentés [ici](https://posit-dev.github.io/great-tables/examples/). Cela se fait en incluant directement le code *html* dans la colonne du *DataFrame*"
      ],
      "id": "f65b6a19-53c4-4c99-b507-25b603c866b4"
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_bar(prop_fill: float, max_width: int, height: int, color: str = \"green\") -> str:\n",
        "    \"\"\"Create divs to represent prop_fill as a bar.\"\"\"\n",
        "    width = round(max_width * prop_fill, 2)\n",
        "    px_width = f\"{width}px\"\n",
        "    return f\"\"\"\\\n",
        "    <div style=\"width: {max_width}px; background-color: lightgrey;\">\\\n",
        "        <div style=\"height:{height}px;width:{px_width};background-color:{color};\"></div>\\\n",
        "    </div>\\\n",
        "    \"\"\"\n",
        "\n",
        "colors = {'empreinte': \"green\", 'revenu': \"red\", 'population': \"blue\"}\n",
        "\n",
        "for variable in ['empreinte', 'revenu', 'population']:\n",
        "    emissions_table[f'raw_perc_{variable}'] = emissions_table[variable]/emissions_table[variable].max()\n",
        "    emissions_table[f'bar_{variable}'] = emissions_table[f'raw_perc_{variable}'].map(\n",
        "        lambda x: create_bar(x, max_width=75, height=20, color = colors[variable])\n",
        "    )"
      ],
      "id": "71b968c8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nous ne gardons que les 5 plus petites empreintes carbone, et les cinq plus importantes."
      ],
      "id": "28da41c3-1886-4a96-a544-748772d4b8e2"
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "emissions_min = emissions_table.head(5).assign(grp = \"5 départements les moins pollueurs\").reset_index(drop=True)\n",
        "emissions_max = emissions_table.tail(5).assign(grp = \"5 départements les plus pollueurs\").reset_index(drop=True)\n",
        "\n",
        "emissions_table = pd.concat([\n",
        "    emissions_min,\n",
        "    emissions_max\n",
        "])"
      ],
      "id": "a2092569"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Enfin, pour pouvoir utiliser quelques fonctions pratiques pour sélectionner des colonnes à partir de motifs, nous allons convertir les données au format [`Polars`](https://pola.rs/)"
      ],
      "id": "7f409175-1484-4cbc-9a43-cfce27917ea0"
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "import polars as pl\n",
        "emissions_table = pl.from_pandas(emissions_table)"
      ],
      "id": "d37bd872"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":\n",
        "\n",
        ":"
      ],
      "id": "37dcb25b-77be-4c7d-9c31-24b9cb4e07aa"
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start from here\n",
        "from great_tables import GT\n",
        "GT(emissions_table, groupname_col=\"grp\", rowname_col=\"dep\")"
      ],
      "id": "e22cc5d8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Le tableau à obtenir:\n",
        "\n",
        "Grâce à celui-ci, on peut déjà comprendre que notre définition\n",
        "de l’empreinte carbone est certainement défaillante. Il apparaît\n",
        "peu plausible que les habitants du 77 aient une empreinte 500 fois\n",
        "supérieure à celle de Paris intra-muros. La raison principale ?\n",
        "On n’est pas sur un concept d’émissions à la consommation mais à la\n",
        "production, ce qui pénalise les espaces industriels ou les espaces\n",
        "avec des aéroports…\n",
        "\n",
        "Pour aller plus loin sur la construction de tableaux\n",
        "avec `great_tables`, vous pouvez répliquer\n",
        "cet [exercice](https://rgeo.linogaliana.fr/exercises/eval.html)\n",
        "de production de tableaux électoraux\n",
        "que j’ai proposé pour un cours de `R` avec `gt`, l’équivalent\n",
        "de `great_tables` pour `R`.\n",
        "\n",
        "# 7. `Pandas`: vers la pratique et au-delà\n",
        "\n",
        "## 7.1 `Pandas` dans une chaine d’opérations\n",
        "\n",
        "En général, dans un projet, le nettoyage de données va consister en un ensemble de\n",
        "méthodes appliquées à un `DataFrame` ou alors une `Serie` lorsqu’on travaille exclusivement sur une colonne.\n",
        "Autrement dit, ce qui est généralement attendu lorsqu’on fait du `Pandas` c’est d’avoir une chaîne qui prend un `DataFrame` en entrée et ressort ce même `DataFrame` enrichi, ou une version agrégée de celui-ci, en sortie.\n",
        "\n",
        "Cette manière de procéder est le coeur de la syntaxe `dplyr` en `R` mais n’est pas forcément native en `Pandas` selon les opérations qu’on désire mettre en oeuvre. En effet, la manière naturelle de mettre à jour un *dataframe* en `Pandas` passe souvent par une syntaxe du type:"
      ],
      "id": "0aa538f9-7b34-4504-9d1e-2856dfab66d6"
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "data = [[8000, 1000], [9500, np.nan], [5000, 2000]]\n",
        "df = pd.DataFrame(data, columns=['salaire', 'autre_info'])\n",
        "df['salaire_net'] = df['salaire']*0.8"
      ],
      "id": "498b9b36"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En `SQL` on pourrait directement mettre à jour notre base de données avec la nouvelle colonne :\n",
        "\n",
        "``` sql\n",
        "SELECT *, salaire*0.8 AS salaire_net FROM df\n",
        "```\n",
        "\n",
        "L’écosystème du *tidyverse* en `R`, l’équivalent de `Pandas`, fonctionne selon la même logique que SQL de mise à jour de table. On ferait en effet la commande suivante avec `dplyr`:\n",
        "\n",
        "``` r\n",
        "df %>% mutate(salaire_net = salaire*0.8)\n",
        "```\n",
        "\n",
        "Techniquement on pourrait faire ceci avec un `assign` en `Pandas`"
      ],
      "id": "4485d27b-4d3f-4235-8a45-ca0c7d86c12c"
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df.drop(\"salaire_net\", axis = \"columns\")\n",
        "df = df.assign(salaire_net = lambda s: s['salaire']*0.8)"
      ],
      "id": "7cd9e10a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cependant cette syntaxe `assign` n’est pas très naturelle. Il est nécessaire de lui passer une *lambda function* qui attend comme *input* un `DataFrame` là où on voudrait une colonne. Il ne s’agit donc pas vraiment d’une syntaxe lisible et pratique.\n",
        "\n",
        "Il est néanmoins possible d’enchaîner des opérations sur des jeux de données grâce aux [*pipes*](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pipe.html). Ceux-ci reprennent la même philosophie que celle de `dplyr`, elle-même inspirée du *pipe* Linux.\n",
        "Cette approche permettra de rendre plus lisible le code en définissant des fonctions effectuant des opérations sur une ou plusieurs colonnes d’un DataFrame. Le premier argument à indiquer à la fonction est le `DataFrame`, les autres sont ceux permettant de contrôler son comportement"
      ],
      "id": "da2a56b4-adf5-4751-822e-09c77f002551"
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calcul_salaire_net(df: pd.DataFrame, col: str, taux: float = 0.8):\n",
        "  df[\"salaire_net\"] = df[col]*taux\n",
        "  return df"
      ],
      "id": "e896e478"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ce qui transforme notre chaine de production en"
      ],
      "id": "8ae17539-583f-4179-bd30-8c1de4b68357"
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [],
      "source": [
        "(\n",
        "  df\n",
        "  .pipe(calcul_salaire_net, \"salaire\")\n",
        ")"
      ],
      "id": "4217a400"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.2 Quelques limites sur la syntaxe de `Pandas`\n",
        "\n",
        "Il y a un avant et un après `Pandas` dans l’analyse de données en `Python`. Sans ce *package* ô combien pratique `Python`, malgré toutes les forces de ce langage, aurait eu du mal à s’installer dans le paysage de l’analyse de données. Cependant, si `Pandas` propose une syntaxe cohérente sur de nombreux aspects, elle n’est pas parfaite non plus. Les paradigmes plus récents d’analyse de données en `Python` ont d’ailleurs parfois l’ambition de corriger ces imperfections syntaxiques là.\n",
        "\n",
        "Parmi les points les plus génants au quoditien il y a le besoin de régulièrement faire des `reset_index` lorsqu’on construit des statistiques descriptives. En effet, il peut être dangereux de garder des indices qu’on ne contrôle pas bien car, sans attention de notre part lors des phases de *merge*, ils peuvent être utilisés à mauvais escient par `Pandas` pour joindre les données ce qui peut provoquer des suprises.\n",
        "\n",
        "`Pandas` est extrêmement bien fait pour restructurer des données du format *long* to *wide* ou *wide* to *long*. Cependant, ce n’est pas la seule manière de restructurer un jeu de données qu’on peut vouloir mettre en oeuvre. Il arrive régulièrement qu’on désire comparer la valeur d’une observation à celle d’un groupe à laquelle elle appartient. C’est notamment particulièrement utile dans une phase d’analyse des anomalies, valeurs aberrantes ou lors d’une investigation de détection de fraude. De manière native, en `Pandas`, il faut construire une statistique agrégée par groupe et refaire un *merge* aux données initiales par le biais de la variable de groupe. C’est un petit peu fastidieux:"
      ],
      "id": "abd2662a-fe38-48bd-8afd-5f2bae32fd6a"
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [],
      "source": [
        "emissions_moyennes = emissions.groupby(\"dep\").agg({\"Agriculture\": \"mean\"}).reset_index()\n",
        "emissions_enrichies = (\n",
        "  emissions\n",
        "  .merge(emissions_moyennes, on = \"dep\", suffixes = ['', '_moyenne_dep'])\n",
        ")\n",
        "emissions_enrichies['relatives'] = emissions_enrichies[\"Agriculture\"]/emissions_enrichies[\"Agriculture_moyenne_dep\"]\n",
        "emissions_enrichies.head()"
      ],
      "id": "6228de3c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dans le *tidyverse*, cette opération en deux temps pourrait être faite en une seule étape, ce qui est plus pratique\n",
        "\n",
        "``` r\n",
        "emissions %>%\n",
        "  group_by(dep) %>%\n",
        "  mutate(relatives = Agriculture/mean(Agriculture))\n",
        "```\n",
        "\n",
        "Ce n’est pas si grave mais cela alourdit la longueur des chaines de traitement faites en `Pandas` et donc la charge de maintenance pour les faire durer dans le temps.\n",
        "\n",
        "De manière plus générale, les chaînes de traitement `Pandas` peuvent être assez verbeuses, car il faut régulièrement redéfinir le `DataFrame` qu’on utilise plutôt que simplement les colonnes. Par exemple, pour faire un filtre sur les lignes et les colonnes, il faudra faire:"
      ],
      "id": "630624c8-19cd-4f22-8b55-bf00f3b92590"
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [],
      "source": [
        "(\n",
        "  emissions\n",
        "  .loc[\n",
        "    (emissions[\"dep\"] == \"12\") & (emissions[\"Routier\"]>500), ['INSEE commune', 'Commune']\n",
        "  ]\n",
        "  .head(5)\n",
        ")"
      ],
      "id": "ef6cc7a9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En SQL on pourrait se contenter de faire référence aux colonnes dans le filter\n",
        "\n",
        "``` sql\n",
        "SELECT \"INSEE commune\", 'Commune'\n",
        "FROM emissions\n",
        "WHERE dep==\"12\" AND Routier>500\n",
        "```\n",
        "\n",
        "Dans le *tidyverse* (`R`) on pourrait aussi faire ceci simplement\n",
        "\n",
        "``` r\n",
        "df %>%\n",
        "  filter(dep==\"12\", Routier>500) %>%\n",
        "  select(`INSEE commune`, `Commune`)\n",
        "```\n",
        "\n",
        "# 8. Les autres paradigmes\n",
        "\n",
        "Ces deux chapitres ont permis d’explorer en profondeur la richesse de l’écosystème `Pandas` qui est un indispensable dans la boite à outil du *data scientist*. Malgré toutes les limites que nous avons pu évoquer, et les solutions alternatives que nous allons présenter, `Pandas` reste LE *package* central de l’écosystème de la donnée avec `Python`. Nous allons voir dans les prochains chapitres son intégration native à l’écosystème `Scikit` pour le *machine learning* ou l’extension de `Pandas` aux données spatiales avec `GeoPandas`.\n",
        "\n",
        "Les autres solutions techniques que nous allons ici évoquer peuvent être pertinentes si on désire traiter des volumes de données importants ou si on désire utiliser des syntaxes alternatives.\n",
        "\n",
        "Les principales alternatives à `Pandas` sont [`Polars`](https://pola.rs/), [`DuckDB`](https://duckdb.org/) et [`Spark`](https://spark.apache.org/docs/latest/api/python/index.html). Il existe également [`Dask`](https://www.dask.org/), une librairie pour paralléliser des traitements écris en `Pandas`.\n",
        "\n",
        "## 8.1 `Polars`\n",
        "\n",
        "`Polars` est certainement le paradigme le plus inspiré de `Pandas`, jusqu’au choix du nom. La première différence fondamentale est dans les couches internes utilisées. `Polars` s’appuie sur l’implémentation `Rust` de `Arrow` là où `Pandas` s’appuie sur `Numpy` ce qui est facteur de perte de performance. Cela permet à `Polars` d’être plus efficace sur de gros volumes de données, d’autant que de nombreuses opérations sont parallélisées et reposent sur l’évaluation différées (*lazy evaluation*) un principe de programmation qui permet d’optimiser les requêtes pour ne pas les exécuter dans l’ordre de définition mais dans un ordre logique plus optimal.\n",
        "\n",
        "Une autre force de `Polars` est la syntaxe plus cohérente, qui bénéficie du recul d’une quinzaine d’années d’existence de `Pandas` et d’une petite dizaine d’années de `dplyr` (le *package* de manipulation de données au sein du paradigme du *tidyverse* en `R`). Pour reprendre l’exemple précédent, il n’est plus nécessaire de forcer la référence au *DataFrame*, dans une chaîne d’exécution toutes les références ultérieures seront faites au regard du *DataFrame* de départ"
      ],
      "id": "ef585644-b2c1-4353-9cff-e8736fcb23d7"
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [],
      "source": [
        "import polars as pl\n",
        "emissions_polars = pl.from_pandas(emissions)\n",
        "(\n",
        "  emissions_polars\n",
        "  .filter(pl.col(\"dep\") == \"12\", pl.col(\"Routier\") > 500)\n",
        "  .select('INSEE commune', 'Commune')\n",
        "  .head(5)\n",
        ")"
      ],
      "id": "f2f9be96"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pour découvrir `Polars`, de nombreuses ressources en ligne sont accessibles, notamment [ce *notebook*](https://github.com/InseeFrLab/ssphub/blob/main/post/polars/polars-tuto.ipynb) construit pour le réseau des *data scientists* de la statistique publique.\n",
        "\n",
        "## 8.2 `DuckDB`\n",
        "\n",
        "*DuckDB* est le nouveau venu dans l’écosystème de l’analyse de données repoussant les limites des données pouvant être traitées avec `Python` sans passer par des outils *big data* comme `Spark`.\n",
        "*DuckDB* est la quintessence d’un nouveau paradigme, celui du [*“Big data is dead”*](https://motherduck.com/blog/big-data-is-dead/), où on peut traiter des données de volumétrie importante sans recourir à des infrastructures imposantes.\n",
        "\n",
        "Outre sa grande efficacité, puisqu’avec *DuckDB* on peut traiter des données d’une volumétrie supérieure à la mémoire vive de l’ordinateur ou du serveur, *DuckDB* présente l’avantage de proposer une syntaxe uniforme quelle que soit le langage qui appelle *DuckDB* (`Python`, `R`, `C++` ou `Javascript`). *DuckDB* privilégie la syntaxe SQL pour traiter les données avec de nombreuses fonctions pré-implementées pour simplifier certaines transformations de données (par exemple pour les [données textuelles](https://duckdb.org/docs/sql/functions/char.html), les [données temporelles](https://duckdb.org/docs/sql/functions/time), etc.).\n",
        "\n",
        "Par rapport à d’autres systèmes s’appuyant sur SQL, comme [`PostGreSQL`](https://www.bing.com/search?go=Rechercher&q=PostGreSQL&qs=ds&form=QBRE), `DuckDB` est très simple d’installation, ce n’est qu’une librairie `Python` là où beaucoup d’outils comme `PostGreSQL` nécessite une infrastructure adaptée.\n",
        "\n",
        "Pour reprendre l’exemple précédent, on peut utiliser directement le code SQL précédent"
      ],
      "id": "1cce4745-34c3-4227-a388-42b14c090001"
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "import duckdb\n",
        "duckdb.sql(\n",
        "  \"\"\"\n",
        "  SELECT \"INSEE commune\", \"Commune\"\n",
        "  FROM emissions\n",
        "  WHERE dep=='12' AND Routier>500\n",
        "  LIMIT 5\n",
        "  \"\"\")"
      ],
      "id": "c633ff53"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ici la clause `FROM emissions` vient du fait qu’on peut directement exécuter du SQL depuis un objet `Pandas` par le biais de `DuckDB`. Si on fait la lecture directement dans la requête, celle-ci se complexifie un petit peu mais la logique est la même"
      ],
      "id": "4097fa89-0471-4ccb-ad99-b60dee513fd4"
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "import duckdb\n",
        "duckdb.sql(\n",
        "  f\"\"\"\n",
        "  SELECT \"INSEE commune\", \"Commune\"\n",
        "  FROM read_csv_auto(\"{url}\")\n",
        "  WHERE\n",
        "    substring(\"INSEE commune\",1,2)=='12'\n",
        "    AND\n",
        "    Routier>500\n",
        "  LIMIT 5\n",
        "  \"\"\")"
      ],
      "id": "c18b7eed"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Le rendu du *DataFrame* est légèrement différent de `Pandas` car, comme `Polars` et de nombreux systèmes de traitement de données volumineuses, `DuckDB` repose sur l’évaluation différée et donc ne présente en *display* qu’un échantillon de données.\n",
        "`DuckDB` et `Polars` sont d’ailleurs très bien intégrés l’un à l’autre. On peut très bien faire du SQL sur un objet `Polars` via `DuckDB` ou appliquer des fonctions `Polars` sur un objet initialement lu avec `DuckDB`.\n",
        "\n",
        "L’un des intérêts de `DuckDB` est son excellente intégration avec l’écosystème `Parquet`, le format de données déjà mentionné qui devient un standard dans le partage de données (il s’agit, par exemple, de la pierre angulaire du partage de données sur la plateforme *HuggingFace*). Pour en savoir plus sur `DuckDB` et découvrir son intérêt pour lire les données du recensement de la population française, vous pouvez consulter [ce post de blog](https://ssphub.netlify.app/post/parquetrp/).\n",
        "\n",
        "## 8.3 `Spark` et le *big data*\n",
        "\n",
        "`DuckDB` a repoussé les frontières du *big data* qu’on peut définir comme le volume de données à partir duquel on ne peut plus traiter celles-ci sur une machine sans mettre en oeuvre une stratégie de parallélisation.\n",
        "\n",
        "Néanmoins, pour les données très volumineuses, `Python` est très bien armé grâce à la librairie [`PySpark`](https://spark.apache.org/docs/latest/api/python/index.html). Celle-ci est une API en Python pour le langage `Spark`, un langage *big data* basé sur Scala. Ce paradigme est construit sur l’idée que les utilisateurs de `Python` y accèdent par le biais de *cluster* avec de nombreux noeuds pour traiter la donnée de manière parallèle. Celle-ci sera lue par blocs, qui seront traités en parallèle en fonction du nombre de noeuds parallèles. L’API DataFrame de `Spark` présente une syntaxe proche de celle des paradigmes précédents avec une ingénieurie plus complexe en arrière-plan liée à la parallélisation native."
      ],
      "id": "12f7647a-dc71-4695-8126-f152ba20b8fe"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/home/runner/work/python-datascientist/python-datascientist/.venv/share/jupyter/kernels/python3"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  }
}